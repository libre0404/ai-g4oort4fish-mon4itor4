import asyncio
import json
import os
import random
from datetime import datetime
from urllib.parse import urlencode

from playwright.async_api import (
    Response,
    TimeoutError as PlaywrightTimeoutError,
    async_playwright,
)
from .stealth_helper import StealthManager  # â† å·²æ·»åŠ 

from src.ai_handler import (
    download_all_images,
    get_ai_analysis,
    send_ntfy_notification,
    cleanup_task_images,
)
from src.config import (
    AI_DEBUG_MODE,
    API_URL_PATTERN,
    DETAIL_API_URL_PATTERN,
    LOGIN_IS_EDGE,
    RUN_HEADLESS,
    RUNNING_IN_DOCKER,
    STATE_FILE,
)
from src.parsers import (
    _parse_search_results_json,
    _parse_user_items_data,
    calculate_reputation_from_ratings,
    parse_ratings_data,
    parse_user_head_data,
)
from src.utils import (
    format_registration_days,
    get_link_unique_key,
    random_sleep,
    safe_get,
    save_to_jsonl,
    log_time,
)
from src.optimization import DelayConfig, UserAgentManager, IPBlockerDetector
from src.config import get_random_user_agent


async def scrape_user_profile(context, user_id: str) -> dict:
    print(f"   -> å¼€å§‹é‡‡é›†ç”¨æˆ·ID: {user_id} çš„å®Œæ•´ä¿¡æ¯...")
    profile_data = {}

    # ã€ç¬¬ 2 å¤„ã€‘æ”¹è¿™è¡Œ + ã€ç¬¬ 3 å¤„ã€‘æ–°å¢ä¸€è¡Œ
    page = await context.new_page(**StealthManager.get_context_config())
    await StealthManager.apply_stealth_async(page)

    # ä¸ºå„é¡¹å¼‚æ­¥ä»»åŠ¡å‡†å¤‡Futureå’Œæ•°æ®å®¹å™¨
    head_api_future = asyncio.get_event_loop().create_future()

    all_items, all_ratings = [], []
    stop_item_scrolling, stop_rating_scrolling = asyncio.Event(), asyncio.Event()

    async def handle_response(response: Response):
        # æ•è·å¤´éƒ¨æ‘˜è¦API
        if "mtop.idle.web.user.page.head" in response.url and not head_api_future.done():
            try:
                head_api_future.set_result(await response.json())
                print(f"      [APIæ•è·] ç”¨æˆ·å¤´éƒ¨ä¿¡æ¯... æˆåŠŸ")
            except Exception as e:
                if not head_api_future.done():
                    head_api_future.set_exception(e)

        # æ•è·å•†å“åˆ—è¡¨API
        elif "mtop.idle.web.xyh.item.list" in response.url:
            try:
                data = await response.json()
                all_items.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] å•†å“åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_items)} ä»¶")
                if not data.get('data', {}).get('nextPage', True):
                    stop_item_scrolling.set()
            except Exception as e:
                stop_item_scrolling.set()

        # æ•è·è¯„ä»·åˆ—è¡¨API
        elif "mtop.idle.web.trade.rate.list" in response.url:
            try:
                data = await response.json()
                all_ratings.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] è¯„ä»·åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_ratings)} æ¡")
                if not data.get('data', {}).get('nextPage', True):
                    stop_rating_scrolling.set()
            except Exception as e:
                stop_rating_scrolling.set()

    page.on("response", handle_response)

    try:
        # --- ä»»åŠ¡1: å¯¼èˆªå¹¶é‡‡é›†å¤´éƒ¨ä¿¡æ¯ ---
        await page.goto(
            f"https://www.goofish.com/personal?userId={user_id}",
            wait_until="domcontentloaded",
            timeout=20000,
        )
        head_data = await asyncio.wait_for(head_api_future, timeout=15)
        profile_data = await parse_user_head_data(head_data)

        # --- ä»»åŠ¡2: æ»šåŠ¨åŠ è½½æ‰€æœ‰å•†å“ (é»˜è®¤é¡µé¢) ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„å•†å“åˆ—è¡¨...")
        await random_sleep(2, 4)  # ç­‰å¾…ç¬¬ä¸€é¡µå•†å“APIå®Œæˆ
        while not stop_item_scrolling.is_set():
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            try:
                await asyncio.wait_for(stop_item_scrolling.wait(), timeout=8)
            except asyncio.TimeoutError:
                print("      [æ»šåŠ¨è¶…æ—¶] å•†å“åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                break
        profile_data["å–å®¶å‘å¸ƒçš„å•†å“åˆ—è¡¨"] = await _parse_user_items_data(all_items)

        # --- ä»»åŠ¡3: ç‚¹å‡»å¹¶é‡‡é›†æ‰€æœ‰è¯„ä»· ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„è¯„ä»·åˆ—è¡¨...")
        rating_tab_locator = page.locator("//div[text()='ä¿¡ç”¨åŠè¯„ä»·']/ancestor::li")
        if await rating_tab_locator.count() > 0:
            await rating_tab_locator.click()
            await random_sleep(3, 5)  # ç­‰å¾…ç¬¬ä¸€é¡µè¯„ä»·APIå®Œæˆ

            while not stop_rating_scrolling.is_set():
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                try:
                    await asyncio.wait_for(stop_rating_scrolling.wait(), timeout=8)
                except asyncio.TimeoutError:
                    print("      [æ»šåŠ¨è¶…æ—¶] è¯„ä»·åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                    break

            profile_data['å–å®¶æ”¶åˆ°çš„è¯„ä»·åˆ—è¡¨'] = await parse_ratings_data(all_ratings)
            reputation_stats = await calculate_reputation_from_ratings(all_ratings)
            profile_data.update(reputation_stats)
        else:
            print("      [è­¦å‘Š] æœªæ‰¾åˆ°è¯„ä»·é€‰é¡¹å¡ï¼Œè·³è¿‡è¯„ä»·é‡‡é›†ã€‚")

    except Exception as e:
        print(f"   [é”™è¯¯] é‡‡é›†ç”¨æˆ· {user_id} ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        page.remove_listener("response", handle_response)
        await page.close()
        print(f"   -> ç”¨æˆ· {user_id} ä¿¡æ¯é‡‡é›†å®Œæˆã€‚")

    return profile_data


async def scrape_xianyu(task_config: dict, debug_limit: int = 0):
    """
    ã€æ ¸å¿ƒæ‰§è¡Œå™¨ã€‘
    æ ¹æ®å•ä¸ªä»»åŠ¡é…ç½®ï¼Œå¼‚æ­¥çˆ¬å–é—²é±¼å•†å“æ•°æ®ï¼Œå¹¶å¯¹æ¯ä¸ªæ–°å‘ç°çš„å•†å“è¿›è¡Œå®æ—¶çš„ã€ç‹¬ç«‹çš„AIåˆ†æå’Œé€šçŸ¥ã€‚
    """
    keyword = task_config['keyword']
    max_pages = task_config.get('max_pages', 1)
    personal_only = task_config.get('personal_only', False)
    min_price = task_config.get('min_price')
    max_price = task_config.get('max_price')
    ai_prompt_text = task_config.get('ai_prompt_text', '')

    processed_item_count = 0
    stop_scraping = False

    processed_links = set()
    output_filename = os.path.join("jsonl", f"{keyword.replace(' ', '_')}_full_data.jsonl")
    if os.path.exists(output_filename):
        print(f"LOG: å‘ç°å·²å­˜åœ¨æ–‡ä»¶ {output_filename}ï¼Œæ­£åœ¨åŠ è½½å†å²è®°å½•ä»¥å»é‡...")
        try:
            with open(output_filename, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        link = record.get('å•†å“ä¿¡æ¯', {}).get('å•†å“é“¾æ¥', '')
                        if link:
                            processed_links.add(get_link_unique_key(link))
                    except json.JSONDecodeError:
                        print(f"   [è­¦å‘Š] æ–‡ä»¶ä¸­æœ‰ä¸€è¡Œæ— æ³•è§£æä¸ºJSONï¼Œå·²è·³è¿‡ã€‚")
            print(f"LOG: åŠ è½½å®Œæˆï¼Œå·²è®°å½• {len(processed_links)} ä¸ªå·²å¤„ç†è¿‡çš„å•†å“ã€‚")
        except IOError as e:
            print(f"   [è­¦å‘Š] è¯»å–å†å²æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        print(f"LOG: è¾“å‡ºæ–‡ä»¶ {output_filename} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")

    async with async_playwright() as p:
        if LOGIN_IS_EDGE:
            # ã€ç¬¬ 4 å¤„ã€‘Edge + StealthManager
            browser = await p.chromium.launch(
                headless=RUN_HEADLESS,
                channel="msedge",
                **StealthManager.get_launch_config(headless=RUN_HEADLESS),
            )
        else:
            if RUNNING_IN_DOCKER:
                # ã€ç¬¬ 5 å¤„ã€‘Docker æ¨¡å¼ + StealthManager
                browser = await p.chromium.launch(
                    headless=RUN_HEADLESS,
                    **StealthManager.get_launch_config(headless=RUN_HEADLESS),
                )
            else:
                # ã€ç¬¬ 6 å¤„ã€‘æœ¬åœ° Chrome + StealthManager
                browser = await p.chromium.launch(
                    headless=RUN_HEADLESS,
                    channel="chrome",
                    **StealthManager.get_launch_config(headless=RUN_HEADLESS),
                )

        # ã€ç¬¬ 7 å¤„ã€‘context å¢åŠ  Stealth + UAï¼ˆåç»­ä¼šç»“åˆ DelayConfig/UserAgentManager åšè¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        random_ua = get_random_user_agent()
        print(f"ğŸ”„ ä½¿ç”¨User-Agent: {random_ua[:80]}...")

        context = await browser.new_context(
            storage_state=STATE_FILE,
            user_agent=random_ua,
            **StealthManager.get_context_config(),
        )

        # ã€ç¬¬ 8 å¤„ã€‘new_page ä¹Ÿå¸¦ Stealth é…ç½®
        page = await context.new_page(**StealthManager.get_context_config())
        # ã€ç¬¬ 9 å¤„ã€‘åº”ç”¨å¼‚æ­¥ Stealth
        await StealthManager.apply_stealth_async(page)

        try:
            log_time("æ­¥éª¤ 1 - ç›´æ¥å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µ...")
            # ä½¿ç”¨ 'q' å‚æ•°æ„å»ºæ­£ç¡®çš„æœç´¢URLï¼Œå¹¶è¿›è¡ŒURLç¼–ç 
            params = {'q': keyword}
            search_url = f"https://www.goofish.com/search?{urlencode(params)}"
            log_time(f"ç›®æ ‡URL: {search_url}")

            # ä½¿ç”¨ expect_response åœ¨å¯¼èˆªçš„åŒæ—¶æ•è·åˆå§‹æœç´¢çš„APIæ•°æ®
            async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                await page.goto(search_url, wait_until="domcontentloaded", timeout=60000)

            initial_response = await response_info.value

            # ç­‰å¾…é¡µé¢åŠ è½½å‡ºå…³é”®ç­›é€‰å…ƒç´ ï¼Œä»¥ç¡®è®¤å·²æˆåŠŸè¿›å…¥æœç´¢ç»“æœé¡µ
            await page.wait_for_selector('text=æ–°å‘å¸ƒ', timeout=15000)

            # --- æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯å¼¹çª— ---
            baxia_dialog = page.locator("div.baxia-dialog-mask")
            middleware_widget = page.locator("div.J_MIDDLEWARE_FRAME_WIDGET")
            try:
                await baxia_dialog.wait_for(state='visible', timeout=2000)
                print("\n==================== CRITICAL BLOCK DETECTED ====================")
                print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯å¼¹çª— (baxia-dialog)ï¼Œæ— æ³•ç»§ç»­æ“ä½œã€‚")
                print("è¿™é€šå¸¸æ˜¯å› ä¸ºæ“ä½œè¿‡äºé¢‘ç¹æˆ–è¢«è¯†åˆ«ä¸ºæœºå™¨äººã€‚")
                print("å»ºè®®ï¼š")
                print("1. åœæ­¢è„šæœ¬ä¸€æ®µæ—¶é—´å†è¯•ã€‚")
                print("2. (æ¨è) åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® RUN_HEADLESS=falseï¼Œä»¥éæ— å¤´æ¨¡å¼è¿è¡Œï¼Œè¿™æœ‰åŠ©äºç»•è¿‡æ£€æµ‹ã€‚")
                print(f"ä»»åŠ¡ '{keyword}' å°†åœ¨æ­¤å¤„ä¸­æ­¢ã€‚")
                print("===================================================================")
                await browser.close()
                return processed_item_count
            except PlaywrightTimeoutError:
                pass

            try:
                await middleware_widget.wait_for(state='visible', timeout=2000)
                print("\n==================== CRITICAL BLOCK DETECTED ====================")
                print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯å¼¹çª— (J_MIDDLEWARE_FRAME_WIDGET)ï¼Œæ— æ³•ç»§ç»­æ“ä½œã€‚")
                print("è¿™é€šå¸¸æ˜¯å› ä¸ºæ“ä½œè¿‡äºé¢‘ç¹æˆ–è¢«è¯†åˆ«ä¸ºæœºå™¨äººã€‚")
                print("å»ºè®®ï¼š")
                print("1. åœæ­¢è„šæœ¬ä¸€æ®µæ—¶é—´å†è¯•ã€‚")
                print("2. (æ¨è) æ›´æ–°ç™»å½•çŠ¶æ€æ–‡ä»¶ï¼Œç¡®ä¿ç™»å½•çŠ¶æ€æœ‰æ•ˆã€‚")
                print("3. é™ä½ä»»åŠ¡æ‰§è¡Œé¢‘ç‡ï¼Œé¿å…è¢«è¯†åˆ«ä¸ºæœºå™¨äººã€‚")
                print(f"ä»»åŠ¡ '{keyword}' å°†åœ¨æ­¤å¤„ä¸­æ­¢ã€‚")
                print("===================================================================")
                await browser.close()
                return processed_item_count
            except PlaywrightTimeoutError:
                pass
            # --- ç»“æŸæ–°å¢ ---

            try:
                await page.click("div[class*='closeIconBg']", timeout=3000)
                print("LOG: å·²å…³é—­å¹¿å‘Šå¼¹çª—ã€‚")
            except PlaywrightTimeoutError:
                print("LOG: æœªæ£€æµ‹åˆ°å¹¿å‘Šå¼¹çª—ã€‚")

            final_response = None
            log_time("æ­¥éª¤ 2 - åº”ç”¨ç­›é€‰æ¡ä»¶...")
            await page.click('text=æ–°å‘å¸ƒ')
            await random_sleep(2, 4)  # åŸæ¥æ˜¯ (1.5, 2.5)
            async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                await page.click('text=æœ€æ–°')
                await random_sleep(4, 7)  # åŸæ¥æ˜¯ (3, 5)
            final_response = await response_info.value

            if personal_only:
                async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                    await page.click('text=ä¸ªäººé—²ç½®')
                    await random_sleep(4, 6)  # åŸæ¥æ˜¯ asyncio.sleep(5)
                final_response = await response_info.value

            if min_price or max_price:
                price_container = page.locator('div[class*="search-price-input-container"]').first
                if await price_container.is_visible():
                    if min_price:
                        await price_container.get_by_placeholder("Â¥").first.fill(min_price)
                        await random_sleep(1, 2.5)  # åŸæ¥æ˜¯ asyncio.sleep(5)
                    if max_price:
                        await price_container.get_by_placeholder("Â¥").nth(1).fill(max_price)
                        await random_sleep(1, 2.5)  # åŸæ¥æ˜¯ asyncio.sleep(5)

                    async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                        await page.keyboard.press('Tab')
                        await random_sleep(4, 7)  # åŸæ¥æ˜¯ asyncio.sleep(5)
                    final_response = await response_info.value
                else:
                    print("LOG: è­¦å‘Š - æœªæ‰¾åˆ°ä»·æ ¼è¾“å…¥å®¹å™¨ã€‚")

            log_time("æ‰€æœ‰ç­›é€‰å·²å®Œæˆï¼Œå¼€å§‹å¤„ç†å•†å“åˆ—è¡¨...")

            current_response = final_response if final_response and final_response.ok else initial_response
            for page_num in range(1, max_pages + 1):
                if stop_scraping:
                    break
                log_time(f"å¼€å§‹å¤„ç†ç¬¬ {page_num}/{max_pages} é¡µ ...")

                if page_num > 1:
                    next_btn = page.locator("[class*='search-pagination-arrow-right']:not([class*='disabled'])")
                    if not await next_btn.count():
                        log_time("å·²åˆ°è¾¾æœ€åä¸€é¡µï¼Œæœªæ‰¾åˆ°å¯ç”¨çš„â€˜ä¸‹ä¸€é¡µâ€™æŒ‰é’®ï¼Œåœæ­¢ç¿»é¡µã€‚")
                        break
                    try:
                        async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=20000) as response_info:
                            await next_btn.click()
                            await random_sleep(5, 8)  # åŸæ¥æ˜¯ (1.5, 3.5)
                        current_response = await response_info.value
                    except PlaywrightTimeoutError:
                        log_time(f"ç¿»é¡µåˆ°ç¬¬ {page_num} é¡µè¶…æ—¶ï¼Œåœæ­¢ç¿»é¡µã€‚")
                        break

                if not (current_response and current_response.ok):
                    log_time(f"ç¬¬ {page_num} é¡µå“åº”æ— æ•ˆï¼Œè·³è¿‡ã€‚")
                    continue

                basic_items = await _parse_search_results_json(await current_response.json(), f"ç¬¬ {page_num} é¡µ")
                if not basic_items:
                    break

                total_items_on_page = len(basic_items)
                for i, item_data in enumerate(basic_items, 1):
                    if debug_limit > 0 and processed_item_count >= debug_limit:
                        log_time(f"å·²è¾¾åˆ°è°ƒè¯•ä¸Šé™ ({debug_limit})ï¼Œåœæ­¢è·å–æ–°å•†å“ã€‚")
                        stop_scraping = True
                        break

                    unique_key = get_link_unique_key(item_data["å•†å“é“¾æ¥"])
                    if unique_key in processed_links:
                        log_time(f"[é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å•†å“ '{item_data['å•†å“æ ‡é¢˜'][:20]}...' å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                        continue

                    log_time(f"[é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å‘ç°æ–°å•†å“ï¼Œè·å–è¯¦æƒ…: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                    await random_sleep(3, 6)  # åŸæ¥æ˜¯ (2, 4)

                    detail_page = await context.new_page(**StealthManager.get_context_config())
                    await StealthManager.apply_stealth_async(detail_page)
                    try:
                        async with detail_page.expect_response(lambda r: DETAIL_API_URL_PATTERN in r.url, timeout=25000) as detail_info:
                            await detail_page.goto(item_data["å•†å“é“¾æ¥"], wait_until="domcontentloaded", timeout=25000)

                        detail_response = await detail_info.value
                        if detail_response.ok:
                            detail_json = await detail_response.json()

                            ret_string = str(await safe_get(detail_json, 'ret', default=[]))
                            if "FAIL_SYS_USER_VALIDATE" in ret_string:
                                print("\n==================== CRITICAL BLOCK DETECTED ====================")
                                print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯ (FAIL_SYS_USER_VALIDATE)ï¼Œç¨‹åºå°†ç»ˆæ­¢ã€‚")
                                long_sleep_duration = random.randint(3, 60)
                                print(f"ä¸ºé¿å…è´¦æˆ·é£é™©ï¼Œå°†æ‰§è¡Œä¸€æ¬¡é•¿æ—¶é—´ä¼‘çœ  ({long_sleep_duration} ç§’) åå†é€€å‡º...")
                                await asyncio.sleep(long_sleep_duration)
                                print("é•¿æ—¶é—´ä¼‘çœ ç»“æŸï¼Œç°åœ¨å°†å®‰å…¨é€€å‡ºã€‚")
                                print("===================================================================")
                                stop_scraping = True
                                break

                            item_do = await safe_get(detail_json, 'data', 'itemDO', default={})
                            seller_do = await safe_get(detail_json, 'data', 'sellerDO', default={})

                            reg_days_raw = await safe_get(seller_do, 'userRegDay', default=0)
                            registration_duration_text = format_registration_days(reg_days_raw)

                            # --- æå–èŠéº»ä¿¡ç”¨ + å›¾ç‰‡åˆ—è¡¨ ---
                            zhima_credit_text = await safe_get(seller_do, 'zhimaLevelInfo', 'levelName')

                            image_infos = await safe_get(item_do, 'imageInfos', default=[])
                            if image_infos:
                                all_image_urls = [img.get('url') for img in image_infos if img.get('url')]
                                if all_image_urls:
                                    item_data['å•†å“å›¾ç‰‡åˆ—è¡¨'] = all_image_urls
                                    item_data['å•†å“ä¸»å›¾é“¾æ¥'] = all_image_urls[0]

                            item_data['â€œæƒ³è¦â€äººæ•°'] = await safe_get(
                                item_do, 'wantCnt', default=item_data.get('â€œæƒ³è¦â€äººæ•°', 'NaN')
                            )
                            item_data['æµè§ˆé‡'] = await safe_get(item_do, 'browseCnt', default='-')

                            # è°ƒç”¨æ ¸å¿ƒå‡½æ•°é‡‡é›†å–å®¶ä¿¡æ¯
                            user_profile_data = {}
                            user_id = await safe_get(seller_do, 'sellerId')
                            if user_id:
                                user_profile_data = await scrape_user_profile(context, str(user_id))
                            else:
                                print("   [è­¦å‘Š] æœªèƒ½ä»è¯¦æƒ…APIä¸­è·å–åˆ°å–å®¶IDã€‚")

                            user_profile_data['å–å®¶èŠéº»ä¿¡ç”¨'] = zhima_credit_text
                            user_profile_data['å–å®¶æ³¨å†Œæ—¶é•¿'] = registration_duration_text

                            final_record = {
                                "çˆ¬å–æ—¶é—´": datetime.now().isoformat(),
                                "æœç´¢å…³é”®å­—": keyword,
                                "ä»»åŠ¡åç§°": task_config.get('task_name', 'Untitled Task'),
                                "å•†å“ä¿¡æ¯": item_data,
                                "å–å®¶ä¿¡æ¯": user_profile_data,
                            }

                            # --- å®æ—¶ AI åˆ†æ + é€šçŸ¥ ---
                            from src.config import SKIP_AI_ANALYSIS

                            if SKIP_AI_ANALYSIS:
                                log_time("ç¯å¢ƒå˜é‡ SKIP_AI_ANALYSIS å·²è®¾ç½®ï¼Œè·³è¿‡AIåˆ†æå¹¶ç›´æ¥å‘é€é€šçŸ¥...")
                                image_urls = item_data.get('å•†å“å›¾ç‰‡åˆ—è¡¨', [])
                                downloaded_image_paths = await download_all_images(
                                    item_data['å•†å“ID'],
                                    image_urls,
                                    task_config.get('task_name', 'default'),
                                )

                                for img_path in downloaded_image_paths:
                                    try:
                                        if os.path.exists(img_path):
                                            os.remove(img_path)
                                            print(f"   [å›¾ç‰‡] å·²åˆ é™¤ä¸´æ—¶å›¾ç‰‡æ–‡ä»¶: {img_path}")
                                    except Exception as e:
                                        print(f"   [å›¾ç‰‡] åˆ é™¤å›¾ç‰‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                                log_time("å•†å“å·²è·³è¿‡AIåˆ†æï¼Œå‡†å¤‡å‘é€é€šçŸ¥...")
                                await send_ntfy_notification(item_data, "å•†å“å·²è·³è¿‡AIåˆ†æï¼Œç›´æ¥é€šçŸ¥")
                            else:
                                log_time(f"å¼€å§‹å¯¹å•†å“ #{item_data['å•†å“ID']} è¿›è¡Œå®æ—¶AIåˆ†æ...")
                                image_urls = item_data.get('å•†å“å›¾ç‰‡åˆ—è¡¨', [])
                                downloaded_image_paths = await download_all_images(
                                    item_data['å•†å“ID'],
                                    image_urls,
                                    task_config.get('task_name', 'default'),
                                )

                                ai_analysis_result = None
                                if ai_prompt_text:
                                    try:
                                        ai_analysis_result = await get_ai_analysis(
                                            final_record,
                                            downloaded_image_paths,
                                            prompt_text=ai_prompt_text,
                                        )
                                        if ai_analysis_result:
                                            final_record['ai_analysis'] = ai_analysis_result
                                            log_time(
                                                f"AIåˆ†æå®Œæˆã€‚æ¨èçŠ¶æ€: {ai_analysis_result.get('is_recommended')}"
                                            )
                                        else:
                                            final_record['ai_analysis'] = {
                                                'error': 'AI analysis returned None after retries.'
                                            }
                                    except Exception as e:
                                        print(f"   -> AIåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                                        final_record['ai_analysis'] = {'error': str(e)}
                                else:
                                    print("   -> ä»»åŠ¡æœªé…ç½®AI promptï¼Œè·³è¿‡åˆ†æã€‚")

                                for img_path in downloaded_image_paths:
                                    try:
                                        if os.path.exists(img_path):
                                            os.remove(img_path)
                                            print(f"   [å›¾ç‰‡] å·²åˆ é™¤ä¸´æ—¶å›¾ç‰‡æ–‡ä»¶: {img_path}")
                                    except Exception as e:
                                        print(f"   [å›¾ç‰‡] åˆ é™¤å›¾ç‰‡æ–‡ä»¶æ—¶å‡ºé”™: {e}")

                                if ai_analysis_result and ai_analysis_result.get('is_recommended'):
                                    log_time("å•†å“è¢«AIæ¨èï¼Œå‡†å¤‡å‘é€é€šçŸ¥...")
                                    await send_ntfy_notification(
                                        item_data, ai_analysis_result.get("reason", "æ— ")
                                    )
                            # --- END AI åˆ†æ ---

                            await save_to_jsonl(final_record, keyword)

                            processed_links.add(unique_key)
                            processed_item_count += 1
                            log_time(f"å•†å“å¤„ç†æµç¨‹å®Œæ¯•ã€‚ç´¯è®¡å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“ã€‚")

                            log_time("[åçˆ¬] æ‰§è¡Œä¸€æ¬¡ä¸»è¦çš„éšæœºå»¶è¿Ÿä»¥æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆé—´éš”...")
                            await random_sleep(15, 30)  # åŸæ¥æ˜¯ (8, 15)

                        else:
                            print(
                                f"   é”™è¯¯: è·å–å•†å“è¯¦æƒ…APIå“åº”å¤±è´¥ï¼ŒçŠ¶æ€ç : {detail_response.status}"
                            )
                            if AI_DEBUG_MODE:
                                print(
                                    f"--- [DETAIL DEBUG] FAILED RESPONSE from {item_data['å•†å“é“¾æ¥']} ---"
                                )
                                try:
                                    print(await detail_response.text())
                                except Exception as e:
                                    print(f"æ— æ³•è¯»å–å“åº”å†…å®¹: {e}")
                                print("----------------------------------------------------")

                    except PlaywrightTimeoutError:
                        print("   é”™è¯¯: è®¿é—®å•†å“è¯¦æƒ…é¡µæˆ–ç­‰å¾…APIå“åº”è¶…æ—¶ã€‚")
                    except Exception as e:
                        print(f"   é”™è¯¯: å¤„ç†å•†å“è¯¦æƒ…æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    finally:
                        await detail_page.close()
                        await random_sleep(2, 4)  # åŸæ¥æ˜¯ (1, 2.5)

                # --- é¡µé—´é•¿ä¼‘æ¯ ---
                if not stop_scraping and page_num < max_pages:
                    print(f"--- ç¬¬ {page_num} é¡µå¤„ç†å®Œæ¯•ï¼Œå‡†å¤‡ç¿»é¡µã€‚æ‰§è¡Œä¸€æ¬¡é¡µé¢é—´çš„é•¿æ—¶ä¼‘æ¯... ---")
                    await random_sleep(25, 50)

        except PlaywrightTimeoutError as e:
            print(f"\næ“ä½œè¶…æ—¶é”™è¯¯: é¡µé¢å…ƒç´ æˆ–ç½‘ç»œå“åº”æœªåœ¨è§„å®šæ—¶é—´å†…å‡ºç°ã€‚\n{e}")
        except Exception as e:
            print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        finally:
            log_time("ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œæµè§ˆå™¨å°†åœ¨5ç§’åè‡ªåŠ¨å…³é—­...")
            await asyncio.sleep(5)
            if debug_limit:
                input("æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            await browser.close()

    cleanup_task_images(task_config.get('task_name', 'default'))

    return processed_item_count
